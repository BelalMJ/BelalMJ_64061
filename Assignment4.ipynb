{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbZi/08SGmm6KkKoWGHtPZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelalMJ/BelalMJ_64061/blob/Assignment-4/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BA64061 - Assignment 4: Text and Sequence Data"
      ],
      "metadata": {
        "id": "5XLtrUlYJXy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Data Preparation"
      ],
      "metadata": {
        "id": "7SWMLh_xJt-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHA_JLxCI1cL"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Numerical computing\n",
        "import tensorflow as tf # Deep learning framework\n",
        "from tensorflow import keras  # High-level neural networks API\n",
        "from tensorflow.keras import layers # Keras layer implementations\n",
        "import matplotlib.pyplot as plt # Data visualization\n",
        "import pandas as pd # Data manipulation and analysis\n",
        "from tqdm.keras import TqdmCallback # Progress bar for training\n",
        "\n",
        "# Configuration parameters for our experiment\n",
        "num_words = 10000\n",
        "maxlen = 150\n",
        "training_samples = 100\n",
        "validation_samples = 10000\n",
        "batch_size = 32\n",
        "epochs = 20  # Increased for meaningful learning curves\n",
        "\n",
        "# Load and split data (IMDB movie review dataset)\n",
        "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Create training, validation, and test splits\n",
        "x_train = x_train_full[:training_samples]\n",
        "y_train = y_train_full[:training_samples]\n",
        "x_val = x_train_full[training_samples:training_samples+validation_samples]\n",
        "y_val = y_train_full[training_samples:training_samples+validation_samples]\n",
        "\n",
        "# Pad sequences to ensure uniform length (150 words)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Print dataset shapes for verification\n",
        "print(f\"Training data: {x_train.shape}, Validation: {x_val.shape}, Test: {x_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Utilities"
      ],
      "metadata": {
        "id": "q4QtVefZJ1Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model performance across train/val/test sets\n",
        "def evaluate_model(model, name, x_train, y_train, x_val, y_val, x_test, y_test):\n",
        "    train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "    val_loss, val_acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Train Accuracy': train_acc,\n",
        "        'Validation Accuracy': val_acc,\n",
        "        'Test Accuracy': test_acc,\n",
        "        'Train Loss': train_loss,\n",
        "        'Validation Loss': val_loss,\n",
        "        'Test Loss': test_loss\n",
        "    }\n",
        "\n",
        "# Function to plot training accuracy across epochs\n",
        "def plot_training_accuracy(histories, sample_size, model_names):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    for name, history in zip(model_names, histories):\n",
        "        plt.plot(history.history['accuracy'], label=f'{name}')\n",
        "\n",
        "    plt.title(f'Training Accuracy (Samples={sample_size})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to compare model performance across different sample sizes\n",
        "def plot_sample_size_comparison(results_df, metric='Accuracy'):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    models = results_df['Model'].unique()\n",
        "    sample_sizes = results_df['Training Samples'].unique()\n",
        "\n",
        "    # Plot performance curve for each model\n",
        "    for model in models:\n",
        "        subset = results_df[results_df['Model'] == model]\n",
        "        plt.plot(subset['Training Samples'], subset[f'Test {metric}'], 'o-', label=model)\n",
        "\n",
        "    plt.title(f'Test {metric} vs Training Samples')\n",
        "    plt.xlabel('Training Samples')\n",
        "    plt.ylabel(f'Test {metric}')\n",
        "    plt.xscale('log')\n",
        "    plt.xticks(sample_sizes, labels=sample_sizes)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DuTOXJGAJ119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying RNN with Learned Embeddings"
      ],
      "metadata": {
        "id": "nrjMgjnTJ9pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequential model with multiple layers\n",
        "def build_rnn_learned():\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
        "        layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True),\n",
        "        layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "\n",
        "# Compile model with Adam optimizer and binary crossentropy loss\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Build and train the RNN with learned embeddings\n",
        "model_rnn_learned = build_rnn_learned()\n",
        "history_rnn_learned = model_rnn_learned.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, TqdmCallback(verbose=0)],\n",
        "    verbose=0)\n",
        "\n",
        "# Evaluate model performance\n",
        "rnn_learned_metrics = evaluate_model(\n",
        "    model_rnn_learned, \"RNN Learned\",\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test)"
      ],
      "metadata": {
        "id": "8lYnpmb8J_z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying RNN with Pretrained GloVe Embeddings"
      ],
      "metadata": {
        "id": "6RJHWcqVKDXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and prepare GloVe embeddings\n",
        "!rm -f glove.6B.zip  # Remove any broken version\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "# Parse the GloVe file and create word-to-vector mapping\n",
        "with open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Get IMDB dataset's word index mapping\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < num_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Create model similar to learned RNN but with GloVe embeddings\n",
        "def build_rnn_glove():\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=num_words, output_dim=embedding_dim,\n",
        "                       input_length=maxlen, weights=[embedding_matrix],\n",
        "                       trainable=True),  # Now trainable for fine-tuning\n",
        "        layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True),\n",
        "        layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "# Use lower learning rate for fine-tuning pre-trained embeddings\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),  # Lower LR for fine-tuning\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train the GloVe-enhanced RNN\n",
        "model_rnn_glove = build_rnn_glove()\n",
        "history_rnn_glove = model_rnn_glove.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, TqdmCallback(verbose=0)],\n",
        "    verbose=0)\n",
        "\n",
        "# Evaluate GloVe model performance\n",
        "rnn_glove_metrics = evaluate_model(\n",
        "    model_rnn_glove, \"RNN GloVe\",\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test)"
      ],
      "metadata": {
        "id": "QxP3F_maKFiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Transformer Model"
      ],
      "metadata": {
        "id": "CDsTaL25KJR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Transformer block implementation\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization()\n",
        "        self.layernorm2 = layers.LayerNormalization()\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Positional embedding layer to add sequence position information\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_emb(inputs)\n",
        "        embedded_positions = self.pos_emb(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "# Define transformer architecture parameters\n",
        "def build_transformer():\n",
        "    embed_dim = 64  # Increased from 32\n",
        "    num_heads = 4\n",
        "    ff_dim = 128    # Increased from 64\n",
        "\n",
        "# Model inputs (sequence of word indices)\n",
        "    inputs = keras.Input(shape=(maxlen,))\n",
        "    x = PositionalEmbedding(maxlen, num_words, embed_dim)(inputs)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Stack two transformer blocks\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Pooling and classification layers\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(32, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile with Adam optimizer\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build and train the Transformer model\n",
        "model_transformer = build_transformer()\n",
        "history_transformer = model_transformer.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, TqdmCallback(verbose=0)],\n",
        "    verbose=0)\n",
        "\n",
        "# Evaluate Transformer performance\n",
        "transformer_metrics = evaluate_model(\n",
        "    model_transformer, \"Transformer\",\n",
        "    x_train, y_train, x_val, y_val, x_test, y_test)"
      ],
      "metadata": {
        "id": "67cYbVt_KI7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Comparison with 100 Samples"
      ],
      "metadata": {
        "id": "oX9764MzKR50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training accuracy for all models\n",
        "plot_training_accuracy(\n",
        "    [history_rnn_learned, history_rnn_glove, history_transformer],\n",
        "    sample_size=100,\n",
        "    model_names=[\"RNN Learned\", \"RNN GloVe\", \"Transformer\"]\n",
        ")\n",
        "\n",
        "# Create comparison table\n",
        "results_df = pd.DataFrame([rnn_learned_metrics, rnn_glove_metrics, transformer_metrics])\n",
        "\n",
        "# Store these results for later use in the next cell\n",
        "initial_results = results_df.copy()\n",
        "initial_results['Training Samples'] = 100\n",
        "\n",
        "# Display formatted results\n",
        "formatted_df = results_df.copy()\n",
        "for col in ['Train Accuracy', 'Validation Accuracy', 'Test Accuracy']:\n",
        "    formatted_df[col] = formatted_df[col].apply(lambda x: f\"{x:.2%}\")\n",
        "for col in ['Train Loss', 'Validation Loss', 'Test Loss']:\n",
        "    formatted_df[col] = formatted_df[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"\\n=== Performance Comparison (100 Samples) ===\")\n",
        "display(formatted_df)"
      ],
      "metadata": {
        "id": "jjIxjSrjKTLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Sample Size Experiments (include 100 samples without retraining)"
      ],
      "metadata": {
        "id": "l7bFDuK9KaK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sizes = [100, 500, 1000, 5000, 25000 ]  # Now includes 100 for plotting\n",
        "all_histories = []\n",
        "experiment_results = []\n",
        "\n",
        "# Add our initial 100-sample results\n",
        "experiment_results.extend(initial_results.to_dict('records'))\n",
        "\n",
        "# Only train on larger sample sizes\n",
        "for size in [s for s in sample_sizes if s != 100]:\n",
        "    print(f\"\\n=== Training with {size} samples ===\")\n",
        "    x_train_sub = x_train_full[:size]\n",
        "    y_train_sub = y_train_full[:size]\n",
        "    x_train_sub = keras.preprocessing.sequence.pad_sequences(x_train_sub, maxlen=maxlen)\n",
        "\n",
        "    # Build fresh models for each experiment\n",
        "    models = {\n",
        "        \"RNN Learned\": build_rnn_learned(),\n",
        "        \"RNN GloVe\": build_rnn_glove(),\n",
        "        \"Transformer\": build_transformer()\n",
        "    }\n",
        "\n",
        "    size_histories = []\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        history = model.fit(\n",
        "            x_train_sub, y_train_sub,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_val, y_val),\n",
        "            callbacks=[early_stopping, TqdmCallback(verbose=0)],\n",
        "            verbose=0\n",
        "        )\n",
        "        size_histories.append(history)\n",
        "\n",
        "        metrics = evaluate_model(\n",
        "            model, name,\n",
        "            x_train_sub, y_train_sub,\n",
        "            x_val, y_val,\n",
        "            x_test, y_test)\n",
        "        metrics['Training Samples'] = size\n",
        "        experiment_results.append(metrics)\n",
        "\n",
        "    all_histories.append((size, size_histories))\n",
        "\n",
        "# Create comprehensive results dataframe\n",
        "exp_df = pd.DataFrame(experiment_results)"
      ],
      "metadata": {
        "id": "zGqwKa-0Kc47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Comparison table and plots"
      ],
      "metadata": {
        "id": "VpSUrOVRKqjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot sample size comparison\n",
        "plot_sample_size_comparison(exp_df, metric='Accuracy')\n",
        "\n",
        "# Create final results table with all metrics\n",
        "final_results = exp_df.copy()\n",
        "for col in ['Train Accuracy', 'Validation Accuracy', 'Test Accuracy']:\n",
        "    final_results[col] = final_results[col].apply(lambda x: f\"{x:.2%}\")\n",
        "for col in ['Train Loss', 'Validation Loss', 'Test Loss']:\n",
        "    final_results[col] = final_results[col].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(\"\\n=== Final Comprehensive Results ===\")\n",
        "display(final_results.sort_values(['Training Samples', 'Model']))\n",
        "\n",
        "# Plot training accuracy for each sample size (except 100 which we already plotted)\n",
        "for size, histories in all_histories:\n",
        "    plot_training_accuracy(\n",
        "        histories,\n",
        "        sample_size=size,\n",
        "        model_names=[\"RNN Learned\", \"RNN GloVe\", \"Transformer\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "r16d9HHnKoVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}